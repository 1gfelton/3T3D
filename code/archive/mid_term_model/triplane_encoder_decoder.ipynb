{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1599485,"sourceType":"datasetVersion","datasetId":943894}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Triplane Encoder/Decoder\nThis is Midterm Baseline Implementation. The architecture defined by Shue et al., we created two toy prototypes to test training of\nthe triplanar representation.","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade PyMCubes\n!pip install trimesh","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport numpy as np\nimport trimesh\nimport os\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport mcubes\nfrom IPython.display import display","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"markdown","source":"## 1 DATASET LOADING AND PREPROCESSIN","metadata":{}},{"cell_type":"code","source":"#===========================================\n# PART 1: DATASET LOADING AND PREPROCESSING\n#===========================================\n\ndef load_off_file(off_path):\n    \"\"\"Load an OFF file and normalize to [-1, 1]\"\"\"\n    try:\n        # Read OFF file manually\n        with open(off_path, 'r') as f:\n            lines = f.readlines()\n        \n        # Parse header\n        line_idx = 0\n        while line_idx < len(lines) and (lines[line_idx].startswith('#') or lines[line_idx].strip() == ''):\n            line_idx += 1\n        \n        # Check if first line is \"OFF\"\n        if lines[line_idx].strip() == \"OFF\":\n            line_idx += 1\n        elif lines[line_idx].strip().startswith(\"OFF\"):\n            # Sometimes \"OFF\" and counts are on the same line\n            counts = lines[line_idx].strip()[3:].split()\n            if len(counts) == 3:\n                num_vertices, num_faces, _ = map(int, counts)\n                line_idx += 1\n            else:\n                line_idx += 1\n                counts = lines[line_idx].strip().split()\n                num_vertices, num_faces, _ = map(int, counts)\n                line_idx += 1\n        else:\n            raise ValueError(f\"Invalid OFF file format: {off_path}\")\n        \n        # Get counts if not yet parsed\n        if 'num_vertices' not in locals():\n            counts = lines[line_idx].strip().split()\n            num_vertices, num_faces, _ = map(int, counts)\n            line_idx += 1\n        \n        # Read vertices\n        vertices = []\n        for i in range(num_vertices):\n            while line_idx < len(lines) and (lines[line_idx].startswith('#') or lines[line_idx].strip() == ''):\n                line_idx += 1\n            vertex = list(map(float, lines[line_idx].strip().split()))\n            vertices.append(vertex)\n            line_idx += 1\n        \n        # Read faces\n        faces = []\n        for i in range(num_faces):\n            while line_idx < len(lines) and (lines[line_idx].startswith('#') or lines[line_idx].strip() == ''):\n                line_idx += 1\n            face_data = list(map(int, lines[line_idx].strip().split()))\n            # First number is the number of vertices in the face\n            face = face_data[1:face_data[0]+1]\n            faces.append(face)\n            line_idx += 1\n        \n        # Create trimesh object\n        mesh = trimesh.Trimesh(vertices=vertices, faces=faces)\n        \n    except Exception as e:\n        # Try using trimesh's loader instead\n        print(f\"Manual parsing failed: {e}, trying trimesh loader\")\n        try:\n            mesh = trimesh.load(off_path, file_type='off')\n        except Exception as e2:\n            print(f\"Trimesh loading failed: {e2}, creating fallback sphere\")\n            # Fallback to a simple sphere\n            mesh = trimesh.creation.icosphere(radius=0.5, subdivisions=2)\n    \n    # Normalize mesh\n    try:\n        # Safety check for empty mesh\n        if len(mesh.vertices) == 0:\n            print(\"Empty mesh, creating fallback sphere\")\n            mesh = trimesh.creation.icosphere(radius=0.5, subdivisions=2)\n        \n        # Center mesh\n        center = mesh.bounding_box.centroid\n        mesh.vertices -= center\n        \n        # Scale mesh safely\n        extents = mesh.bounding_box.extents\n        max_extent = np.max(extents)\n        \n        if max_extent < 1e-6:  # Very small mesh\n            print(\"Mesh too small, creating fallback sphere\")\n            mesh = trimesh.creation.icosphere(radius=0.5, subdivisions=2)\n        else:\n            scale = 2.0 / max_extent  # Scale to [-1, 1]\n            mesh.vertices *= scale\n    \n    except Exception as e:\n        print(f\"Normalization error: {e}, creating fallback sphere\")\n        mesh = trimesh.creation.icosphere(radius=0.5, subdivisions=2)\n    \n    return mesh\n\ndef load_modelnet40_models(base_path, categories=None, max_models=3, split='train'):\n    \"\"\"Load models from ModelNet40 dataset\"\"\"\n    meshes = []\n    mesh_paths = []\n    \n    # Handle categories\n    if categories is None:\n        categories = [d for d in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, d))]\n    \n    if isinstance(categories, str):\n        categories = [categories]\n    \n    for category in categories:\n        category_path = os.path.join(base_path, category, split)\n        if not os.path.exists(category_path):\n            print(f\"Warning: Path {category_path} does not exist\")\n            continue\n        \n        # Get all OFF files\n        off_files = [f for f in os.listdir(category_path) if f.endswith('.off')]\n        off_files = off_files[:max_models]  # Limit number of models\n        \n        for off_file in off_files:\n            off_path = os.path.join(category_path, off_file)\n            try:\n                mesh = load_off_file(off_path)\n                meshes.append(mesh)\n                mesh_paths.append(off_path)\n                print(f\"Loaded: {off_path}\")\n            except Exception as e:\n                print(f\"Error loading {off_path}: {e}\")\n    \n    return meshes, mesh_paths\n\ndef sample_points_from_mesh(mesh, n_points=5000):\n    \"\"\"Sample points and compute occupancy values\"\"\"\n    try:\n        # Sample surface points\n        surface_points, _ = trimesh.sample.sample_surface(mesh, n_points // 2)\n        \n        # Add noise to create near-surface points\n        near_surface_points = surface_points + np.random.normal(0, 0.01, size=surface_points.shape)\n        \n        # Generate random volume points\n        volume_points = np.random.uniform(-1, 1, size=(n_points // 2, 3))\n        \n        # Combine all points\n        all_points = np.vstack([surface_points, near_surface_points, volume_points])\n        \n        # Compute occupancy (inside/outside)\n        occupancy = np.zeros(len(all_points), dtype=np.float32)\n        \n        # Process in batches to avoid memory issues\n        batch_size = 1000\n        for i in tqdm(range(0, len(all_points), batch_size), desc=\"Computing occupancy\"):\n            batch = all_points[i:i+batch_size]\n            try:\n                # Try using signed distance\n                if hasattr(mesh, 'proximity'):\n                    occupancy[i:i+batch_size] = (mesh.proximity.signed_distance(batch) <= 0).astype(np.float32)\n                else:\n                    # Fall back to contains test\n                    for j, point in enumerate(batch):\n                        occupancy[i+j] = float(mesh.contains([point])[0])\n            except:\n                # If both methods fail, use distance to nearest point on mesh\n                for j, point in enumerate(batch):\n                    closest_point = mesh.nearest.vertex(point)[1]\n                    occupancy[i+j] = float(np.linalg.norm(point - closest_point) < 0.05)\n        \n        return all_points, occupancy.reshape(-1, 1)\n    \n    except Exception as e:\n        print(f\"Error sampling points: {e}, creating fallback points\")\n        # Create fallback points (sphere)\n        points = np.random.uniform(-1, 1, size=(n_points, 3))\n        occupancy = (np.linalg.norm(points, axis=1) < 0.5).astype(np.float32).reshape(-1, 1)\n        return points, occupancy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Encode & Decode triplane","metadata":{}},{"cell_type":"markdown","source":"## 2: TRIPLANE REPRESENTATION","metadata":{}},{"cell_type":"code","source":"#===========================================\n# PART 2: TRIPLANE REPRESENTATION\n#===========================================\n\nclass FourierFeatureTransform(nn.Module):\n    \"\"\"Fourier feature mapping\"\"\"\n    def __init__(self, input_channels, mapping_size, scale=1.0):\n        super().__init__()\n        self.input_channels = input_channels\n        self.mapping_size = mapping_size\n        self.B = nn.Parameter(torch.randn((input_channels, mapping_size)) * scale, requires_grad=False)\n    \n    def forward(self, x):\n        # x: [batch_size, n_points, input_channels]\n        batch_size, n_points, channels = x.shape\n        # Project and reshape\n        x = (x.reshape(batch_size * n_points, channels) @ self.B).reshape(batch_size, n_points, -1)\n        # Apply sine and cosine\n        x = 2 * np.pi * x\n        return torch.cat([torch.sin(x), torch.cos(x)], dim=-1)\n\nclass MiniTriplane(nn.Module):\n    \"\"\"Triplane representation with MLP decoder\"\"\"\n    def __init__(self, feature_dim=32, resolution=128, output_dim=1):\n        super().__init__()\n        \n        # Create the three feature planes\n        self.embeddings = nn.ParameterList([\n            nn.Parameter(torch.randn(1, feature_dim, resolution, resolution) * 0.001)\n            for _ in range(3)\n        ])\n        \n        # Decoder network\n        self.net = nn.Sequential(\n            FourierFeatureTransform(feature_dim, 64, scale=1),\n            nn.Linear(128, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, 128),\n            nn.ReLU(inplace=True),\n            nn.Linear(128, output_dim),\n        )\n    \n    def sample_plane(self, coords2d, plane):\n        \"\"\"Sample features from a 2D plane safely\"\"\"\n        # Ensure coords are in valid range\n        coords2d = torch.clamp(coords2d, min=-1.0, max=1.0)\n        \n        # Handle NaN values\n        if torch.isnan(coords2d).any():\n            coords2d = torch.nan_to_num(coords2d, nan=0.0)\n        \n        # Reshape for grid_sample\n        grid = coords2d.reshape(coords2d.shape[0], 1, -1, 2)\n        \n        # Ensure plane batch dimension matches grid\n        if plane.shape[0] != grid.shape[0]:\n            plane = plane.repeat(grid.shape[0], 1, 1, 1)\n        \n        # Sample features\n        sampled = F.grid_sample(\n            plane, \n            grid, \n            mode='bilinear', \n            padding_mode='zeros', \n            align_corners=True\n        )\n        \n        # Reshape to [batch_size, n_points, channels]\n        batch_size, channels, _, n_points = sampled.shape\n        sampled = sampled.reshape(batch_size, channels, n_points).permute(0, 2, 1)\n        \n        return sampled\n    \n    def forward(self, coords):\n        \"\"\"Forward pass with safe error handling\"\"\"\n        # coords: [batch_size, n_points, 3]\n        batch_size, n_points, dims = coords.shape\n        \n        # Ensure coordinates are in range [-1, 1]\n        coords = torch.clamp(coords, min=-1.0, max=1.0)\n        \n        try:\n            # Sample from each plane\n            xy_features = self.sample_plane(coords[..., 0:2], self.embeddings[0])  # XY plane\n            yz_features = self.sample_plane(coords[..., 1:3], self.embeddings[1])  # YZ plane\n            xz_features = self.sample_plane(coords[..., ::2], self.embeddings[2])   # XZ plane\n            \n            # Sum features from all planes\n            features = xy_features + yz_features + xz_features\n            \n            # Process through MLP\n            return self.net(features)\n            \n        except Exception as e:\n            print(f\"Error in forward pass: {e}\")\n            # Return zeros as fallback\n            return torch.zeros(batch_size, n_points, 1, device=coords.device)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3: TRAINING THE TRIPLANE ENCODER","metadata":{}},{"cell_type":"code","source":"#===========================================\n# PART 3: TRAINING THE TRIPLANE ENCODER\n#===========================================\n\ndef train_encoder(mesh, output_path=None, epochs=200, feature_dim=32, resolution=128, device=\"cuda\"):\n    \"\"\"Train triplane encoder from mesh\"\"\"\n    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n    print(f\"Using device: {device}\")\n    \n    # Sample points and occupancy\n    print(\"Sampling points from mesh...\")\n    points, occupancy = sample_points_from_mesh(mesh)\n    print(f\"Sampled {len(points)} points\")\n    \n    # Create encoder (triplane + MLP)\n    encoder = MiniTriplane(feature_dim=feature_dim, resolution=resolution).to(device)\n    print(\"Created encoder\")\n    \n    # Optimizer and loss\n    optimizer = optim.Adam(encoder.parameters(), lr=1e-4)\n    criterion = nn.BCEWithLogitsLoss()\n    \n    # Training parameters\n    batch_size = min(10000, len(points))\n    n_batches = len(points) // batch_size\n    \n    # Convert data to tensors\n    points_tensor = torch.FloatTensor(points).to(device)\n    occupancy_tensor = torch.FloatTensor(occupancy).to(device)\n    \n    # Regularization weights\n    lambda_tv = 0.1   # Total variation\n    lambda_l2 = 0.01  # L2 regularization\n    \n    # Training loop\n    print(\"Starting training...\")\n    losses = []\n    \n    for epoch in range(epochs):\n        epoch_loss = 0\n        # Shuffle data\n        perm = torch.randperm(len(points_tensor))\n        \n        for b in range(n_batches):\n            try:\n                # Get batch\n                start_idx = b * batch_size\n                end_idx = min((b + 1) * batch_size, len(points_tensor))\n                idx = perm[start_idx:end_idx]\n                \n                batch_points = points_tensor[idx].unsqueeze(0)  # [1, batch_size, 3]\n                batch_occupancy = occupancy_tensor[idx]\n                \n                # Forward pass\n                pred_occupancy = encoder(batch_points).squeeze(0)\n                \n                # Fix NaN values\n                if torch.isnan(pred_occupancy).any():\n                    pred_occupancy = torch.nan_to_num(pred_occupancy, nan=0.0)\n                \n                # Reconstruction loss\n                recon_loss = criterion(pred_occupancy, batch_occupancy)\n                \n                # Total variation regularization\n                tv_loss = 0\n                for plane in encoder.embeddings:\n                    tv_loss += torch.mean(torch.abs(plane[:, :, 1:, :] - plane[:, :, :-1, :]))\n                    tv_loss += torch.mean(torch.abs(plane[:, :, :, 1:] - plane[:, :, :, :-1]))\n                \n                # L2 regularization\n                l2_loss = sum(torch.sum(plane ** 2) for plane in encoder.embeddings)\n                \n                # Total loss\n                loss = recon_loss + lambda_tv * tv_loss + lambda_l2 * l2_loss\n                \n                # Backpropagation\n                optimizer.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_norm_(encoder.parameters(), max_norm=1.0)\n                optimizer.step()\n                \n                epoch_loss += loss.item()\n                \n            except Exception as e:\n                print(f\"Error in batch {b}: {e}\")\n                continue\n        \n        # Average loss for epoch\n        avg_loss = epoch_loss / n_batches\n        losses.append(avg_loss)\n        \n        # Print progress\n        if (epoch + 1) % 10 == 0:\n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n    \n    # Save triplane features\n    if output_path:\n        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n        features = np.stack([embed.data.cpu().numpy() for embed in encoder.embeddings])\n        np.save(output_path, features)\n        print(f\"Saved triplane features to: {output_path}\")\n        \n        # Visualize features\n        visualize_features(features, f\"{os.path.splitext(output_path)[0]}_features.png\")\n    \n    print(\"Training complete!\")\n    return encoder\n\ndef visualize_features(features, save_path=None):\n    \"\"\"Visualize triplane features\"\"\"\n    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n    plane_names = ['XY Plane', 'YZ Plane', 'XZ Plane']\n    \n    for i in range(3):\n        # Average across feature channels\n        avg_feature = features[i, 0].mean(axis=0)\n        \n        # Plot\n        im = axes[i].imshow(avg_feature, cmap='viridis')\n        axes[i].set_title(f\"{plane_names[i]}\")\n        plt.colorbar(im, ax=axes[i])\n    \n    plt.tight_layout()\n    \n    if save_path:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        plt.savefig(save_path)\n        print(f\"Saved feature visualization to {save_path}\")\n    else:\n        plt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4: MESH GENERATION","metadata":{}},{"cell_type":"code","source":"#===========================================\n# PART 4: MESH GENERATION\n#===========================================\n\ndef create_mesh(model, res=128, threshold=0.0, device=\"cuda\"):\n    \"\"\"Extract mesh from trained model\"\"\"\n    model.eval()\n    \n    try:\n        # Create coordinate grid\n        x = torch.linspace(-1, 1, res)\n        y = torch.linspace(-1, 1, res)\n        z = torch.linspace(-1, 1, res)\n        \n        X, Y, Z = torch.meshgrid(x, y, z, indexing='ij')\n        coords = torch.stack([X, Y, Z], dim=-1).reshape(-1, 3)\n        \n        # Evaluate in batches\n        batch_size = 10000\n        predictions = []\n        \n        with torch.no_grad():\n            for i in range(0, coords.shape[0], batch_size):\n                batch_coords = coords[i:i+batch_size].to(device).unsqueeze(0)\n                batch_output = model(batch_coords).cpu().squeeze()\n                predictions.append(batch_output)\n        \n        # Combine predictions\n        all_predictions = torch.cat(predictions).reshape(res, res, res).numpy()\n        \n        # Extract surface using marching cubes\n        vertices, triangles = mcubes.marching_cubes(all_predictions, threshold)\n        \n        # If no surface found, try different thresholds\n        if len(vertices) == 0 or len(triangles) == 0:\n            for alt_threshold in [-0.2, 0.2, -0.5, 0.5]:\n                vertices, triangles = mcubes.marching_cubes(all_predictions, alt_threshold)\n                if len(vertices) > 0 and len(triangles) > 0:\n                    break\n        \n        # Map vertices back to [-1, 1] space\n        vertices = vertices / (res - 1) * 2 - 1\n        \n        return vertices, triangles\n    \n    except Exception as e:\n        print(f\"Error creating mesh: {e}\")\n        # Create a simple sphere as fallback\n        phi = np.linspace(0, 2 * np.pi, 30)\n        theta = np.linspace(0, np.pi, 20)\n        \n        x = 0.5 * np.outer(np.cos(phi), np.sin(theta)).flatten()\n        y = 0.5 * np.outer(np.sin(phi), np.sin(theta)).flatten()\n        z = 0.5 * np.outer(np.ones_like(phi), np.cos(theta)).flatten()\n        \n        vertices = np.vstack([x, y, z]).T\n        \n        # Create triangles\n        triangles = []\n        for i in range(29):\n            for j in range(19):\n                p1 = i * 20 + j\n                p2 = (i + 1) % 30 * 20 + j\n                p3 = i * 20 + (j + 1) % 20\n                p4 = (i + 1) % 30 * 20 + (j + 1) % 20\n                triangles.append([p1, p2, p3])\n                triangles.append([p2, p4, p3])\n        \n        triangles = np.array(triangles)\n        \n        return vertices, triangles\n\ndef save_obj(vertices, triangles, filename):\n    \"\"\"Save mesh as OBJ file\"\"\"\n    os.makedirs(os.path.dirname(filename), exist_ok=True)\n    \n    with open(filename, 'w') as f:\n        for v in vertices:\n            f.write(f'v {v[0]} {v[1]} {v[2]}\\n')\n        for t in triangles:\n            # OBJ uses 1-indexed vertices\n            f.write(f'f {t[0]+1} {t[1]+1} {t[2]+1}\\n')\n    \n    print(f\"Saved mesh to {filename}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Diffusion","metadata":{}},{"cell_type":"markdown","source":"## 5: DIFFUSION MODEL","metadata":{}},{"cell_type":"code","source":"#===========================================\n# PART 5: DIFFUSION MODEL\n#===========================================\n\n# Diffusion model components\nclass ResBlock(nn.Module):\n    \"\"\"Residual block with time embedding\"\"\"\n    def __init__(self, in_channels, out_channels, time_emb_dim):\n        super().__init__()\n        \n        self.norm1 = nn.GroupNorm(8, in_channels)\n        self.act1 = nn.SiLU()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding=1)\n        \n        # Time embedding projection\n        self.time_mlp = nn.Linear(time_emb_dim, out_channels)\n        \n        self.norm2 = nn.GroupNorm(8, out_channels)\n        self.act2 = nn.SiLU()\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding=1)\n        \n        # Shortcut connection\n        if in_channels != out_channels:\n            self.shortcut = nn.Conv2d(in_channels, out_channels, 1)\n        else:\n            self.shortcut = nn.Identity()\n    \n    def forward(self, x, temb):\n        h = self.norm1(x)\n        h = self.act1(h)\n        h = self.conv1(h)\n        \n        # Add time embedding\n        h = h + self.time_mlp(temb)[:, :, None, None]\n        \n        h = self.norm2(h)\n        h = self.act2(h)\n        h = self.conv2(h)\n        \n        return h + self.shortcut(x)\n\nclass SelfAttention(nn.Module):\n    \"\"\"Self-attention module\"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.norm = nn.GroupNorm(8, channels)\n        self.qkv = nn.Conv2d(channels, channels * 3, 1)\n        self.proj = nn.Conv2d(channels, channels, 1)\n    \n    def forward(self, x):\n        B, C, H, W = x.shape\n        \n        h = self.norm(x)\n        qkv = self.qkv(h)\n        q, k, v = qkv.chunk(3, dim=1)\n        \n        # Reshape for attention\n        q = q.reshape(B, C, H * W).permute(0, 2, 1)  # [B, HW, C]\n        k = k.reshape(B, C, H * W)  # [B, C, HW]\n        v = v.reshape(B, C, H * W).permute(0, 2, 1)  # [B, HW, C]\n        \n        # Attention calculation\n        scale = 1.0 / (C ** 0.5)\n        attn = torch.bmm(q, k) * scale  # [B, HW, HW]\n        attn = F.softmax(attn, dim=-1)\n        \n        out = torch.bmm(attn, v)  # [B, HW, C]\n        out = out.permute(0, 2, 1).reshape(B, C, H, W)\n        \n        return self.proj(out) + x\n\nclass Downsample(nn.Module):\n    \"\"\"Downsampling with strided convolution\"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.conv = nn.Conv2d(channels, channels, 3, stride=2, padding=1)\n    \n    def forward(self, x):\n        return self.conv(x)\n\nclass Upsample(nn.Module):\n    \"\"\"Upsampling with interpolation + convolution\"\"\"\n    def __init__(self, channels):\n        super().__init__()\n        self.conv = nn.Conv2d(channels, channels, 3, padding=1)\n    \n    def forward(self, x):\n        x = F.interpolate(x, scale_factor=2, mode=\"nearest\")\n        return self.conv(x)\n\ndef timestep_embedding(timesteps, dim, max_period=10000):\n    \"\"\"Sinusoidal position embedding for timesteps\"\"\"\n    half = dim // 2\n    freqs = torch.exp(-torch.log(torch.tensor(max_period)) * torch.arange(start=0, end=half, dtype=torch.float32) / half)\n    args = timesteps[:, None].float() * freqs[None]\n    embedding = torch.cat([torch.cos(args), torch.sin(args)], dim=-1)\n    \n    if dim % 2:\n        embedding = torch.cat([embedding, embedding[:, :1]], dim=-1)\n    \n    return embedding\n\nclass TriplaneUNet(nn.Module):\n    \"\"\"UNet architecture for diffusion model\"\"\"\n    def __init__(self, in_channels, base_channels=64, channel_mults=(1, 2, 4, 8), time_emb_dim=256):\n        super().__init__()\n        \n        # Time embedding\n        self.time_embed = nn.Sequential(\n            nn.Linear(base_channels, time_emb_dim),\n            nn.SiLU(),\n            nn.Linear(time_emb_dim, time_emb_dim)\n        )\n        \n        # Input processing\n        self.input_conv = nn.Conv2d(in_channels, base_channels, 3, padding=1)\n        \n        # Downsampling path\n        self.down_blocks = nn.ModuleList()\n        current_channels = base_channels\n        down_channels = [current_channels]\n        \n        for level, mult in enumerate(channel_mults):\n            out_channels = base_channels * mult\n            \n            # Two ResBlocks per level\n            for _ in range(2):\n                self.down_blocks.append(ResBlock(current_channels, out_channels, time_emb_dim))\n                current_channels = out_channels\n                down_channels.append(current_channels)\n            \n            # Downsampling except at last level\n            if level < len(channel_mults) - 1:\n                self.down_blocks.append(Downsample(current_channels))\n                down_channels.append(current_channels)\n        \n        # Middle blocks\n        self.mid_block1 = ResBlock(current_channels, current_channels, time_emb_dim)\n        self.mid_attn = SelfAttention(current_channels)\n        self.mid_block2 = ResBlock(current_channels, current_channels, time_emb_dim)\n        \n        # Upsampling path\n        self.up_blocks = nn.ModuleList()\n        \n        for level, mult in reversed(list(enumerate(channel_mults))):\n            out_channels = base_channels * mult\n            \n            # Three ResBlocks per level with skip connections\n            for _ in range(3):\n                skip_channels = down_channels.pop()\n                self.up_blocks.append(ResBlock(current_channels + skip_channels, out_channels, time_emb_dim))\n                current_channels = out_channels\n            \n            # Upsampling except at first level\n            if level > 0:\n                self.up_blocks.append(Upsample(current_channels))\n        \n        # Output processing\n        self.norm_out = nn.GroupNorm(8, current_channels)\n        self.act_out = nn.SiLU()\n        self.conv_out = nn.Conv2d(current_channels, in_channels, 3, padding=1)\n    \n    def forward(self, x, timesteps):\n        # Time embedding\n        temb = self.time_embed(timestep_embedding(timesteps, self.time_embed[0].in_features))\n        \n        # Initial convolution\n        h = self.input_conv(x)\n        \n        # Store skip connections\n        skips = [h]\n        \n        # Downsampling\n        for module in self.down_blocks:\n            if isinstance(module, ResBlock):\n                h = module(h, temb)\n            else:  # Downsample\n                h = module(h)\n            skips.append(h)\n        \n        # Middle blocks\n        h = self.mid_block1(h, temb)\n        h = self.mid_attn(h)\n        h = self.mid_block2(h, temb)\n        \n        # Upsampling with skip connections\n        for module in self.up_blocks:\n            if isinstance(module, ResBlock):\n                skip = skips.pop()\n                h = torch.cat([h, skip], dim=1)\n                h = module(h, temb)\n            else:  # Upsample\n                h = module(h)\n        \n        # Output\n        h = self.norm_out(h)\n        h = self.act_out(h)\n        h = self.conv_out(h)\n        \n        return h","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class GaussianDiffusion:\n    \"\"\"Diffusion model for triplane generation\"\"\"\n    def __init__(self, model, timesteps=1000, beta_start=0.0001, beta_end=0.02, device=\"cuda\"):\n        self.model = model\n        self.timesteps = timesteps\n        self.device = device\n        \n        # Define noise schedule\n        self.betas = torch.linspace(beta_start, beta_end, timesteps, device=device)\n        self.alphas = 1.0 - self.betas\n        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n        \n        # Pre-compute coefficients for diffusion and denoising\n        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n        \n        # Posterior variance calculation\n        self.posterior_variance = self.betas * (1.0 - self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        self.posterior_log_variance_clipped = torch.log(\n            torch.cat([self.posterior_variance[1:2], self.posterior_variance[1:]])\n        )\n        self.posterior_mean_coef1 = (\n            self.betas * torch.sqrt(self.alphas_cumprod_prev) / (1.0 - self.alphas_cumprod)\n        )\n        self.posterior_mean_coef2 = (\n            (1.0 - self.alphas_cumprod_prev) * torch.sqrt(self.alphas) / (1.0 - self.alphas_cumprod)\n        )\n    \n    def q_sample(self, x_start, t, noise=None):\n        \"\"\"Forward diffusion process: q(x_t | x_0)\"\"\"\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        \n        # Extract coefficients based on timestep\n        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n        sqrt_one_minus_alphas_cumprod_t = extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n        \n        # Formula: x_t = sqrt(alpha_cumprod_t) * x_0 + sqrt(1-alpha_cumprod_t) * noise\n        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n    \n    def p_losses(self, x_start, t, noise=None):\n        \"\"\"Calculate training loss\"\"\"\n        if noise is None:\n            noise = torch.randn_like(x_start)\n        \n        # Add noise to input\n        x_noisy = self.q_sample(x_start, t, noise=noise)\n        \n        # Predict noise using the model\n        predicted_noise = self.model(x_noisy, t)\n        \n        # Loss is MSE between actual and predicted noise\n        loss = F.mse_loss(predicted_noise, noise)\n        \n        return loss\n    \n    def p_mean_variance(self, x, t):\n        \"\"\"Compute mean and variance for the reverse process\"\"\"\n        # Predict noise\n        pred_noise = self.model(x, t)\n        \n        # Calculate mean\n        sqrt_recip_alphas_t = extract(self.sqrt_recip_alphas, t, x.shape)\n        sqrt_one_minus_alphas_cumprod_t = extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n        \n        model_mean = sqrt_recip_alphas_t * (x - sqrt_one_minus_alphas_cumprod_t * pred_noise)\n        \n        # Calculate variance\n        posterior_variance_t = extract(self.posterior_variance, t, x.shape)\n        posterior_log_variance_t = extract(self.posterior_log_variance_clipped, t, x.shape)\n        \n        return model_mean, posterior_variance_t, posterior_log_variance_t\n    \n    @torch.no_grad()\n    def p_sample(self, x, t):\n        \"\"\"Sample from p(x_{t-1} | x_t)\"\"\"\n        model_mean, _, model_log_variance = self.p_mean_variance(x, t)\n        \n        # No noise at timestep 0\n        if t[0] == 0:\n            return model_mean\n        \n        # Add noise scaled by the variance\n        noise = torch.randn_like(x)\n        return model_mean + torch.exp(0.5 * model_log_variance) * noise\n    \n    @torch.no_grad()\n    def p_sample_loop(self, shape):\n        \"\"\"Generate samples by iterative denoising\"\"\"\n        device = self.device\n        b = shape[0]\n        \n        # Start from pure noise\n        img = torch.randn(shape, device=device)\n        \n        # Iteratively denoise\n        for i in tqdm(reversed(range(0, self.timesteps)), desc=\"Sampling\", total=self.timesteps):\n            img = self.p_sample(img, torch.full((b,), i, device=device, dtype=torch.long))\n        \n        return img\n    \n    @torch.no_grad()\n    def sample(self, batch_size=1, resolution=128, feature_channels=32):\n        \"\"\"Generate triplane samples\"\"\"\n        # Define shape for stacked features\n        channels = feature_channels * 3  # Three planes stacked in channel dimension\n        shape = (batch_size, channels, resolution, resolution)\n        \n        # Generate samples\n        samples = self.p_sample_loop(shape)\n        \n        # Reshape to triplane format [B, 3, 1, C, H, W]\n        result = []\n        for i in range(batch_size):\n            # Split channels into three parts (one for each plane)\n            planes = torch.split(samples[i], feature_channels, dim=0)\n            # Add dimensions to match expected format\n            planes = [p.unsqueeze(0) for p in planes]  # [1, C, H, W]\n            # Stack planes\n            stacked = torch.stack(planes, dim=0)  # [3, 1, C, H, W]\n            result.append(stacked)\n        \n        # Stack batch dimension\n        result = torch.stack(result, dim=0)  # [B, 3, 1, C, H, W]\n        \n        return result\n    \n    def train_step(self, x_batch, optimizer):\n        \"\"\"Perform one training step\"\"\"\n        device = self.device\n        \n        # Select random timesteps\n        t = torch.randint(0, self.timesteps, (x_batch.shape[0],), device=device, dtype=torch.long)\n        \n        # Calculate loss\n        loss = self.p_losses(x_batch, t)\n        \n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        return loss.item()\n\ndef extract(a, t, x_shape):\n    \"\"\"Extract appropriate timestep values and reshape for broadcasting\"\"\"\n    batch_size = t.shape[0]\n    out = a.gather(-1, t).reshape(batch_size, *((1,) * (len(x_shape) - 1)))\n    return out.expand(x_shape)\n\nclass TriplaneDataset(torch.utils.data.Dataset):\n    \"\"\"Dataset for triplane features\"\"\"\n    def __init__(self, features_list):\n        self.features = []\n        \n        for feature in features_list:\n            # Convert to torch tensor if needed\n            if isinstance(feature, np.ndarray):\n                tensor = torch.from_numpy(feature).float()\n            else:\n                tensor = feature\n            \n            # Process based on shape\n            if tensor.ndim == 5 and tensor.shape[0] == 3 and tensor.shape[1] == 1:\n                # Shape: [3, 1, C, H, W]\n                # Stack the three planes along channel dimension\n                C, H, W = tensor.shape[2], tensor.shape[3], tensor.shape[4]\n                planes = [tensor[i, 0] for i in range(3)]  # 3 x [C, H, W]\n                stacked = torch.cat(planes, dim=0)  # [3*C, H, W]\n                self.features.append(stacked)\n            elif tensor.ndim == 4 and tensor.shape[0] == 3:\n                # Shape: [3, C, H, W]\n                stacked = torch.cat([tensor[i] for i in range(3)], dim=0)  # [3*C, H, W]\n                self.features.append(stacked)\n            else:\n                print(f\"Warning: Unsupported feature shape {tensor.shape}\")\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        return self.features[idx]","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 6: TRAINING FUNCTIONS","metadata":{}},{"cell_type":"code","source":"#===========================================\n# PART 6: TRAINING FUNCTIONS\n#===========================================\n\ndef create_triplane_dataset(meshes, mesh_paths, output_dir=\"triplane_features\"):\n    \"\"\"Create triplane features from meshes\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    os.makedirs(os.path.join(output_dir, \"reconstructions\"), exist_ok=True)\n    \n    features_list = []\n    \n    for i, (mesh, path) in enumerate(zip(meshes, mesh_paths)):\n        name = os.path.basename(path).split('.')[0]\n        output_path = os.path.join(output_dir, f\"{name}.npy\")\n        \n        try:\n            print(f\"Processing mesh {i+1}/{len(meshes)}: {name}\")\n            \n            # Train encoder for this mesh\n            encoder = train_encoder(\n                mesh=mesh,\n                output_path=output_path,\n                epochs=200,\n                feature_dim=32,\n                resolution=128,\n                device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n            )\n            \n            # Load features and add to list\n            features = np.load(output_path)\n            features_list.append(features)\n            \n            # Create test reconstruction mesh\n            try:\n                vertices, triangles = create_mesh(\n                    encoder, \n                    res=128, \n                    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n                )\n                save_obj(\n                    vertices, \n                    triangles, \n                    os.path.join(output_dir, \"reconstructions\", f\"{name}.obj\")\n                )\n            except Exception as e:\n                print(f\"Error creating reconstruction mesh: {e}\")\n                \n        except Exception as e:\n            print(f\"Error processing mesh {name}: {e}\")\n    \n    print(f\"Created {len(features_list)} triplane features\")\n    return features_list\n\ndef train_diffusion_model(dataset, epochs=100, batch_size=4, lr=1e-4, device=\"cuda\"):\n    \"\"\"Train the diffusion model on triplane features\"\"\"\n    # Ensure we have data\n    if len(dataset) == 0:\n        print(\"Error: Empty dataset\")\n        return None, None\n    \n    # Create dataloader\n    dataloader = torch.utils.data.DataLoader(\n        dataset, \n        batch_size=min(batch_size, len(dataset)), \n        shuffle=True\n    )\n    \n    # Get input features shape from dataset\n    sample = dataset[0]\n    in_channels = sample.shape[0]  # [3*C, H, W]\n    \n    # Create model\n    model = TriplaneUNet(in_channels=in_channels).to(device)\n    \n    # Create diffusion process\n    diffusion = GaussianDiffusion(model, device=device)\n    \n    # Optimizer\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    \n    # Training loop\n    for epoch in range(epochs):\n        epoch_loss = 0\n        batch_count = 0\n        \n        # Process each batch\n        for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n            try:\n                # Move to device\n                batch = batch.to(device)\n                \n                # Handle NaN values\n                if torch.isnan(batch).any():\n                    batch = torch.nan_to_num(batch, nan=0.0)\n                \n                # Train step\n                loss = diffusion.train_step(batch, optimizer)\n                epoch_loss += loss\n                batch_count += 1\n                \n            except Exception as e:\n                print(f\"Error in batch {batch_idx}: {e}\")\n                continue\n        \n        # Calculate average loss\n        avg_loss = epoch_loss / max(1, batch_count)\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n        \n        # Save checkpoint\n        if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n            checkpoint_dir = \"checkpoints\"\n            os.makedirs(checkpoint_dir, exist_ok=True)\n            \n            checkpoint_path = os.path.join(checkpoint_dir, f\"diffusion_epoch{epoch+1}.pt\")\n            torch.save({\n                'epoch': epoch,\n                'model_state_dict': model.state_dict(),\n                'optimizer_state_dict': optimizer.state_dict(),\n                'loss': avg_loss,\n            }, checkpoint_path)\n            \n            print(f\"Saved checkpoint to {checkpoint_path}\")\n            \n            # Generate a test sample\n            if epoch >= epochs // 2:  # Only generate samples in the latter half of training\n                try:\n                    # Shape information from dataset\n                    resolution = int(np.sqrt(sample.shape[1]))  # Assuming square resolution\n                    feature_channels = in_channels // 3\n                    \n                    # Generate sample\n                    test_sample = diffusion.sample(\n                        batch_size=1, \n                        resolution=resolution, \n                        feature_channels=feature_channels\n                    )\n                    \n                    # Save sample\n                    sample_dir = \"samples\"\n                    os.makedirs(sample_dir, exist_ok=True)\n                    \n                    sample_np = test_sample.cpu().numpy()\n                    np.save(os.path.join(sample_dir, f\"sample_epoch{epoch+1}.npy\"), sample_np)\n                    \n                    # Visualize feature planes\n                    visualize_features(\n                        sample_np[0], \n                        save_path=os.path.join(sample_dir, f\"sample_epoch{epoch+1}.png\")\n                    )\n                    \n                    # Create mesh from sample\n                    try:\n                        # Create decoder\n                        decoder = MiniTriplane(\n                            feature_dim=feature_channels, \n                            resolution=resolution\n                        ).to(device)\n                        \n                        # Load generated features\n                        for j in range(3):\n                            decoder.embeddings[j].data = test_sample[0, j, 0].to(device)\n                        \n                        # Generate mesh\n                        vertices, triangles = create_mesh(decoder, res=128, device=device)\n                        \n                        # Save OBJ\n                        save_obj(\n                            vertices, \n                            triangles, \n                            os.path.join(sample_dir, f\"sample_epoch{epoch+1}.obj\")\n                        )\n                    except Exception as e:\n                        print(f\"Error creating mesh from sample: {e}\")\n                \n                except Exception as e:\n                    print(f\"Error generating sample: {e}\")\n    \n    return model, diffusion","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Experiment","metadata":{}},{"cell_type":"markdown","source":"## 7: FULL PIPELINE","metadata":{}},{"cell_type":"code","source":"#===========================================\n# PART 7: FULL PIPELINE\n#===========================================\n\ndef run_modelnet40_pipeline(modelnet40_path, output_dir=\"results\", \n                            categories=[\"airplane\"], max_models=3, \n                            epochs=50, device=\"cuda\"):\n    \"\"\"Complete pipeline from ModelNet40 to generated 3D models\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"=== Step 1: Loading models from ModelNet40 ===\")\n    meshes, mesh_paths = load_modelnet40_models(\n        modelnet40_path,\n        categories=categories,\n        max_models=max_models,\n        split='train'\n    )\n    \n    # Check if we have models\n    if len(meshes) == 0:\n        print(\"No models loaded! Adding fallback shapes\")\n        \n        # Add simple shapes as fallback\n        box = trimesh.creation.box(extents=[1.0, 1.0, 1.0])\n        sphere = trimesh.creation.icosphere(radius=0.5)\n        cylinder = trimesh.creation.cylinder(radius=0.5, height=1.0)\n        \n        meshes = [box, sphere, cylinder]\n        mesh_paths = [\"box.obj\", \"sphere.obj\", \"cylinder.obj\"]\n    \n    print(f\"Loaded {len(meshes)} models\")\n    \n    print(\"\\n=== Step 2: Creating triplane features ===\")\n    triplane_dir = os.path.join(output_dir, \"triplane_features\")\n    features_list = create_triplane_dataset(meshes, mesh_paths, triplane_dir)\n    \n    # Check if we have features\n    if len(features_list) == 0:\n        print(\"Failed to create triplane features!\")\n        return None, None\n    \n    print(\"\\n=== Step 3: Creating dataset and training diffusion model ===\")\n    dataset = TriplaneDataset(features_list)\n    \n    print(f\"Dataset size: {len(dataset)}\")\n    print(f\"Sample shape: {dataset[0].shape}\")\n    \n    # Train diffusion model\n    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n    model, diffusion = train_diffusion_model(\n        dataset,\n        epochs=epochs,\n        batch_size=min(4, len(dataset)),\n        device=device\n    )\n    \n    if model is None or diffusion is None:\n        print(\"Training failed!\")\n        return None, None\n    \n    print(\"\\n=== Step 4: Generating new 3D models ===\")\n    # Get shape information from dataset\n    sample = dataset[0]\n    resolution = int(np.sqrt(sample.shape[1]))  # Assuming square resolution\n    feature_channels = sample.shape[0] // 3\n    \n    # Generate samples\n    n_samples = 5\n    samples_dir = os.path.join(output_dir, \"generated_samples\")\n    os.makedirs(samples_dir, exist_ok=True)\n    \n    for i in range(n_samples):\n        try:\n            print(f\"Generating sample {i+1}/{n_samples}\")\n            \n            # Generate triplane features\n            sample = diffusion.sample(\n                batch_size=1,\n                resolution=resolution,\n                feature_channels=feature_channels\n            )\n            \n            # Save features\n            sample_np = sample.cpu().numpy()\n            np.save(os.path.join(samples_dir, f\"sample_{i}.npy\"), sample_np)\n            \n            # Visualize features\n            visualize_features(\n                sample_np[0],\n                save_path=os.path.join(samples_dir, f\"sample_{i}_features.png\")\n            )\n            \n            # Generate mesh\n            decoder = MiniTriplane(\n                feature_dim=feature_channels,\n                resolution=resolution\n            ).to(device)\n            \n            # Load features\n            for j in range(3):\n                decoder.embeddings[j].data = sample[0, j, 0].to(device)\n            \n            # Create mesh\n            vertices, triangles = create_mesh(decoder, res=128, device=device)\n            \n            # Save OBJ\n            save_obj(\n                vertices,\n                triangles,\n                os.path.join(samples_dir, f\"sample_{i}.obj\")\n            )\n            \n        except Exception as e:\n            print(f\"Error generating sample {i}: {e}\")\n    \n    print(f\"\\nPipeline complete! Results saved to {output_dir}\")\n    return model, diffusion","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Fixed sample method for the GaussianDiffusion class\n@torch.no_grad()\ndef sample_fixed(self, batch_size=1, resolution=128, feature_channels=32):\n    \"\"\"Generate triplane samples with proper dimension handling\"\"\"\n    # Define shape for stacked features\n    channels = feature_channels * 3  # Three planes stacked in channel dimension\n    shape = (batch_size, channels, resolution, resolution)\n    \n    # Generate samples\n    samples = self.p_sample_loop(shape)\n    \n    # Reshape to triplane format [B, 3, 1, C, H, W]\n    result = []\n    for i in range(batch_size):\n        # Get sample for this batch item\n        sample = samples[i]  # [3*C, H, W]\n        \n        # Split into three equal chunks for each plane\n        # Make sure the split is done correctly along the first dimension\n        chunk_size = sample.shape[0] // 3\n        planes = torch.chunk(sample, 3, dim=0)  # 3 x [C, H, W]\n        \n        # Reshape and add batch dimension for each plane\n        planes_processed = []\n        for plane in planes:\n            # Add extra dimensions to match expected format [1, C, H, W]\n            plane_expanded = plane.unsqueeze(0)\n            planes_processed.append(plane_expanded)\n        \n        # Stack along new dimension to get [3, 1, C, H, W]\n        stacked = torch.stack(planes_processed, dim=0)\n        result.append(stacked)\n    \n    # Stack batch dimension\n    result = torch.stack(result, dim=0)  # [B, 3, 1, C, H, W]\n    \n    return result\n\n# Fixed function to use the sample in the training pipeline\ndef generate_samples_fixed(model, diffusion, output_dir, n_samples=5, resolution=128, feature_channels=32, device=\"cuda\"):\n    \"\"\"Generate samples with proper error handling and dimension fixes\"\"\"\n    samples_dir = os.path.join(output_dir, \"generated_samples\")\n    os.makedirs(samples_dir, exist_ok=True)\n    \n    for i in range(n_samples):\n        try:\n            print(f\"Generating sample {i+1}/{n_samples}\")\n            \n            # Generate triplane features using fixed sampling function\n            sample = diffusion.sample_fixed(\n                batch_size=1,\n                resolution=resolution,\n                feature_channels=feature_channels\n            )\n            \n            # Debug info\n            print(f\"Generated sample shape: {sample.shape}\")\n            \n            # Save features\n            sample_np = sample.cpu().numpy()\n            np.save(os.path.join(samples_dir, f\"sample_{i}.npy\"), sample_np)\n            \n            # Visualize features\n            visualize_features(\n                sample_np[0],\n                save_path=os.path.join(samples_dir, f\"sample_{i}_features.png\")\n            )\n            \n            # Generate mesh\n            decoder = MiniTriplane(\n                feature_dim=feature_channels,\n                resolution=resolution\n            ).to(device)\n            \n            # Carefully load features into decoder with shape checking\n            for j in range(3):\n                if sample[0, j].dim() == 3 and sample[0, j].shape[0] == 1:\n                    # Shape is [1, C, H, W]\n                    decoder.embeddings[j].data = sample[0, j].to(device)\n                else:\n                    # Try to reshape if needed\n                    print(f\"Reshaping plane {j} from shape {sample[0, j].shape}\")\n                    decoder.embeddings[j].data = sample[0, j].reshape(1, feature_channels, resolution, resolution).to(device)\n            \n            # Create mesh\n            vertices, triangles = create_mesh(decoder, res=128, device=device)\n            \n            # Save OBJ\n            save_obj(\n                vertices,\n                triangles,\n                os.path.join(samples_dir, f\"sample_{i}.obj\")\n            )\n            \n        except Exception as e:\n            print(f\"Error generating sample {i}: {e}\")\n            # Print full traceback for debugging\n            import traceback\n            traceback.print_exc()\n\n# Fixed run_modelnet40_pipeline function\ndef run_modelnet40_pipeline_fixed(modelnet40_path, output_dir=\"results\", \n                                 categories=[\"airplane\"], max_models=3, \n                                 epochs=50, device=\"cuda\"):\n    \"\"\"Fixed pipeline from ModelNet40 to generated 3D models\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"=== Step 1: Loading models from ModelNet40 ===\")\n    meshes, mesh_paths = load_modelnet40_models(\n        modelnet40_path,\n        categories=categories,\n        max_models=max_models,\n        split='train'\n    )\n    \n    # Check if we have models\n    if len(meshes) == 0:\n        print(\"No models loaded! Adding fallback shapes\")\n        \n        # Add simple shapes as fallback\n        box = trimesh.creation.box(extents=[1.0, 1.0, 1.0])\n        sphere = trimesh.creation.icosphere(radius=0.5)\n        cylinder = trimesh.creation.cylinder(radius=0.5, height=1.0)\n        \n        meshes = [box, sphere, cylinder]\n        mesh_paths = [\"box.obj\", \"sphere.obj\", \"cylinder.obj\"]\n    \n    print(f\"Loaded {len(meshes)} models\")\n    \n    print(\"\\n=== Step 2: Creating triplane features ===\")\n    triplane_dir = os.path.join(output_dir, \"triplane_features\")\n    features_list = create_triplane_dataset(meshes, mesh_paths, triplane_dir)\n    \n    # Check if we have features\n    if len(features_list) == 0:\n        print(\"Failed to create triplane features!\")\n        return None, None\n    \n    print(\"\\n=== Step 3: Creating dataset and training diffusion model ===\")\n    dataset = TriplaneDataset(features_list)\n    \n    print(f\"Dataset size: {len(dataset)}\")\n    print(f\"Sample shape: {dataset[0].shape}\")\n    \n    # Train diffusion model\n    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n    model, diffusion = train_diffusion_model(\n        dataset,\n        epochs=epochs,\n        batch_size=min(4, len(dataset)),\n        device=device\n    )\n    \n    if model is None or diffusion is None:\n        print(\"Training failed!\")\n        return None, None\n    \n    # Monkey patch the sample method with our fixed version\n    diffusion.sample_fixed = lambda batch_size=1, resolution=128, feature_channels=32: sample_fixed(\n        diffusion, batch_size, resolution, feature_channels\n    )\n    \n    print(\"\\n=== Step 4: Generating new 3D models ===\")\n    # Get shape information from dataset\n    sample = dataset[0]\n    resolution = int(np.sqrt(sample.shape[1]))  # Assuming square resolution\n    feature_channels = sample.shape[0] // 3\n    \n    # Generate samples using fixed function\n    generate_samples_fixed(\n        model, \n        diffusion, \n        output_dir, \n        n_samples=5, \n        resolution=resolution, \n        feature_channels=feature_channels,\n        device=device\n    )\n    \n    print(f\"\\nPipeline complete! Results saved to {output_dir}\")\n    return model, diffusion","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GaussianDiffusionsample\n@torch.no_grad()\ndef fixed_sample(self, batch_size=1, resolution=128, feature_channels=32):\n    \"\"\"Modified sampling method that ensures compatible dimensions for UNet architecture\"\"\"\n    # Ensure resolution is a power of 2 to avoid dimension mismatches in UNet\n    # Find the closest power of 2 that is at least as large as the requested resolution\n    power_of_2 = 2 ** (resolution - 1).bit_length()\n    if power_of_2 != resolution:\n        print(f\"Warning: Adjusting resolution from {resolution} to {power_of_2} (power of 2) for UNet compatibility\")\n        resolution = power_of_2\n    \n    # Define shape for diffusion model output (channels for all three planes combined)\n    channels = feature_channels * 3\n    shape = (batch_size, channels, resolution, resolution)\n    \n    # Generate samples from noise\n    print(f\"Generating sample with shape {shape}\")\n    samples = self.p_sample_loop(shape)\n    print(f\"Generated raw samples with shape {samples.shape}\")\n    \n    # Reshape results into triplane format\n    results = []\n    for i in range(batch_size):\n        # Get current sample\n        sample = samples[i]  # [3*C, H, W]\n        print(f\"Processing sample {i} with shape {sample.shape}\")\n        \n        # Split channels evenly into three parts for each plane\n        chunk_size = sample.shape[0] // 3\n        planes = torch.split(sample, chunk_size, dim=0)  # 3 x [C, H, W]\n        \n        # Debug info\n        print(f\"Split into {len(planes)} planes of shape {planes[0].shape}\")\n        \n        # Process each plane to ensure correct dimensions\n        processed_planes = []\n        for plane in planes:\n            # Add batch dimension to get [1, C, H, W]\n            plane = plane.unsqueeze(0)\n            processed_planes.append(plane)\n        \n        # Stack processed planes to get [3, 1, C, H, W]\n        stacked = torch.stack(processed_planes, dim=0)\n        print(f\"Stacked planes shape: {stacked.shape}\")\n        results.append(stacked)\n    \n    # Stack all batch samples\n    final_result = torch.stack(results, dim=0)  # [B, 3, 1, C, H, W]\n    print(f\"Final output shape: {final_result.shape}\")\n    \n    return final_result\n# \ndef generate_samples(model, diffusion, output_dir, n_samples=5, resolution=128, feature_channels=32, device=\"cuda\"):\n    \"\"\"Generate samples from diffusion model and create 3D models with error handling\"\"\"\n    samples_dir = os.path.join(output_dir, \"generated_samples\")\n    os.makedirs(samples_dir, exist_ok=True)\n    \n    # Replace the sample method with our fixed version\n    diffusion.sample = fixed_sample.__get__(diffusion, type(diffusion))\n    \n    for i in range(n_samples):\n        try:\n            print(f\"\\nGenerating sample {i+1}/{n_samples}\")\n            \n            # Generate triplane features\n            sample = diffusion.sample(\n                batch_size=1,\n                resolution=resolution,\n                feature_channels=feature_channels\n            )\n            \n            # Save and visualize features\n            sample_np = sample.cpu().numpy()\n            np.save(os.path.join(samples_dir, f\"sample_{i}.npy\"), sample_np)\n            \n            print(f\"Feature shape: {sample_np.shape}\")\n            \n            try:\n                # Attempt to visualize\n                if hasattr(sample_np, 'shape') and len(sample_np.shape) >= 5:\n                    visualize_features(\n                        sample_np[0],\n                        save_path=os.path.join(samples_dir, f\"sample_{i}_features.png\")\n                    )\n                else:\n                    print(f\"Cannot visualize features with shape {sample_np.shape}\")\n            except Exception as e:\n                print(f\"Visualization error: {e}\")\n            \n            # Create decoder\n            decoder = MiniTriplane(\n                feature_dim=feature_channels,\n                resolution=resolution\n            ).to(device)\n            \n            # Load features into decoder\n            for j in range(3):\n                try:\n                    if sample.dim() >= 5 and sample.shape[2] == 1:\n                        # If shape is [B, 3, 1, C, H, W]\n                        decoder.embeddings[j].data = sample[0, j].to(device)\n                    else:\n                        # If shape doesn't match, try to reshape\n                        print(f\"Reshaping plane {j}, original shape: {sample[0, j].shape}\")\n                        plane_data = sample[0, j].reshape(1, feature_channels, resolution, resolution)\n                        decoder.embeddings[j].data = plane_data.to(device)\n                except Exception as e:\n                    print(f\"Failed to load plane {j}: {e}\")\n                    # Use random features as fallback\n                    decoder.embeddings[j].data = torch.randn(1, feature_channels, resolution, resolution, device=device) * 0.01\n            \n            # Create mesh\n            try:\n                vertices, triangles = create_mesh(decoder, res=128, device=device)\n                \n                # Save OBJ file\n                obj_path = os.path.join(samples_dir, f\"sample_{i}.obj\")\n                save_obj(vertices, triangles, obj_path)\n                print(f\"Saved OBJ file to: {obj_path}\")\n            except Exception as e:\n                print(f\"Failed to create mesh: {e}\")\n                # Print full error info\n                import traceback\n                traceback.print_exc()\n                \n        except Exception as e:\n            print(f\"Failed to generate sample {i}: {e}\")\n            # Print full error info\n            import traceback\n            traceback.print_exc()\n    \n    print(f\"Completed generating {n_samples} samples\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Fixed: TriplaneDataset","metadata":{}},{"cell_type":"code","source":"class FixedTriplaneDataset(torch.utils.data.Dataset):\n    def __init__(self, features_list, target_resolution=None):\n        self.features = []\n        \n        for feature in features_list:\n            # Convert to torch tensor if needed\n            if isinstance(feature, np.ndarray):\n                tensor = torch.from_numpy(feature).float()\n            else:\n                tensor = feature\n            \n            # Process based on shape\n            if tensor.ndim == 5 and tensor.shape[0] == 3 and tensor.shape[1] == 1:\n                # Shape: [3, 1, C, H, W]\n                C, H, W = tensor.shape[2], tensor.shape[3], tensor.shape[4]\n                \n                # Ensure H and W are compatible with UNet (optional resize)\n                if target_resolution is not None and (H != target_resolution or W != target_resolution):\n                    # Resize each plane\n                    resized_planes = []\n                    for i in range(3):\n                        plane = tensor[i, 0]  # [C, H, W]\n                        plane = F.interpolate(\n                            plane.unsqueeze(0),  # Add batch dim: [1, C, H, W]\n                            size=(target_resolution, target_resolution),\n                            mode='bilinear',\n                            align_corners=True\n                        ).squeeze(0)  # Remove batch dim: [C, target_resolution, target_resolution]\n                        resized_planes.append(plane)\n                    \n                    # Stack resized planes\n                    stacked = torch.cat(resized_planes, dim=0)  # [3*C, target_resolution, target_resolution]\n                else:\n                    # Keep original size\n                    planes = [tensor[i, 0] for i in range(3)]  # 3 x [C, H, W]\n                    stacked = torch.cat(planes, dim=0)  # [3*C, H, W]\n                \n                self.features.append(stacked)\n                \n            elif tensor.ndim == 4 and tensor.shape[0] == 3:\n                # Shape: [3, C, H, W]\n                C, H, W = tensor.shape[1], tensor.shape[2], tensor.shape[3]\n                \n                # Ensure H and W are compatible with UNet (optional resize)\n                if target_resolution is not None and (H != target_resolution or W != target_resolution):\n                    # Resize each plane\n                    resized_planes = []\n                    for i in range(3):\n                        plane = tensor[i]  # [C, H, W]\n                        plane = F.interpolate(\n                            plane.unsqueeze(0),  # Add batch dim: [1, C, H, W]\n                            size=(target_resolution, target_resolution),\n                            mode='bilinear',\n                            align_corners=True\n                        ).squeeze(0)  # Remove batch dim: [C, target_resolution, target_resolution]\n                        resized_planes.append(plane)\n                    \n                    # Stack resized planes\n                    stacked = torch.cat(resized_planes, dim=0)  # [3*C, target_resolution, target_resolution]\n                else:\n                    # Keep original size\n                    stacked = torch.cat([tensor[i] for i in range(3)], dim=0)  # [3*C, H, W]\n                \n                self.features.append(stacked)\n            else:\n                print(f\"Warning: Unsupported feature shape {tensor.shape}\")\n    \n    def __len__(self):\n        return len(self.features)\n    \n    def __getitem__(self, idx):\n        return self.features[idx]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def triplane_diffusion_pipeline(modelnet40_path, output_dir=\"results\", \n                                    categories=[\"airplane\"], max_models=1, \n                                    epochs=30, device=\"cuda\"):\n    \"\"\"Complete triplane diffusion pipeline with fixes for resolution and sampling\"\"\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    # Step 1: Load ModelNet40 models\n    print(\"=== Step 1: Loading ModelNet40 models ===\")\n    meshes, mesh_paths = load_modelnet40_models(\n        modelnet40_path,\n        categories=categories,\n        max_models=max_models,\n        split='train'\n    )\n    \n    # Check if we have models\n    if len(meshes) == 0:\n        print(\"No models loaded, creating default shapes\")\n        # Create default shapes\n        box = trimesh.creation.box(extents=[1.0, 1.0, 1.0])\n        sphere = trimesh.creation.icosphere(radius=0.5)\n        \n        meshes = [box, sphere]\n        mesh_paths = [\"box.obj\", \"sphere.obj\"]\n    \n    print(f\"Loaded {len(meshes)} models\")\n    \n    # Step 2: Create triplane features\n    print(\"\\n=== Step 2: Creating triplane features ===\")\n    triplane_dir = os.path.join(output_dir, \"triplane_features\")\n    features_list = []\n    \n    # Create triplane features for each model\n    for i, (mesh, path_or_name) in enumerate(zip(meshes, mesh_paths)):\n        name = os.path.basename(path_or_name).split('.')[0]\n        output_path = os.path.join(triplane_dir, f\"{name}.npy\")\n        \n        try:\n            print(f\"Processing model {i+1}/{len(meshes)}: {name}\")\n            \n            # Train encoder\n            encoder = train_encoder(\n                mesh=mesh,\n                output_path=output_path,\n                epochs=200,\n                feature_dim=32,\n                resolution=128,  # Use power of 2 for resolution\n                device=device\n            )\n            \n            # Load features\n            features = np.load(output_path)\n            features_list.append(features)\n            \n            # Create reconstruction mesh for validation\n            try:\n                recon_dir = os.path.join(triplane_dir, \"reconstructions\")\n                os.makedirs(recon_dir, exist_ok=True)\n                \n                vertices, triangles = create_mesh(encoder, res=128, device=device)\n                save_obj(\n                    vertices, \n                    triangles, \n                    os.path.join(recon_dir, f\"{name}_recon.obj\")\n                )\n            except Exception as e:\n                print(f\"Failed to create reconstruction mesh: {e}\")\n        \n        except Exception as e:\n            print(f\"Failed to process model {name}: {e}\")\n    \n    if not features_list:\n        print(\"Failed to create triplane features!\")\n        return None, None\n    \n    print(f\"Created {len(features_list)} triplane features\")\n    \n    # Step 3: Create dataset and train diffusion model\n    print(\"\\n=== Step 3: Training diffusion model ===\")\n    \n    # Define target resolution that works with UNet (power of 2)\n    target_resolution = 128  # This ensures compatibility with the UNet architecture\n    \n    # Create fixed dataset with compatible resolution\n    dataset = FixedTriplaneDataset(features_list, target_resolution=target_resolution)\n    print(f\"Dataset size: {len(dataset)}\")\n    if len(dataset) > 0:\n        print(f\"Sample shape: {dataset[0].shape}\")\n    \n    # Train diffusion model\n    device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n    \n    # Function to train diffusion model\n    def train_diffusion_model_fixed(dataset, epochs=30, batch_size=1, lr=1e-4, device=\"cuda\"):\n        \"\"\"Train diffusion model with fixed sample method\"\"\"\n        if len(dataset) == 0:\n            print(\"Error: Empty dataset\")\n            return None, None\n        \n        # Create data loader\n        dataloader = torch.utils.data.DataLoader(\n            dataset, \n            batch_size=min(batch_size, len(dataset)), \n            shuffle=True\n        )\n        \n        # Get input feature shape\n        sample = dataset[0]\n        in_channels = sample.shape[0]\n        print(f\"Input channels: {in_channels}\")\n        \n        # Create model\n        model = TriplaneUNet(in_channels=in_channels).to(device)\n        \n        # Create diffusion process\n        diffusion = GaussianDiffusion(model, device=device)\n        \n        # Optimizer\n        optimizer = optim.Adam(model.parameters(), lr=lr)\n        \n        # Training loop\n        for epoch in range(epochs):\n            epoch_loss = 0\n            batch_count = 0\n            \n            # Process each batch\n            for batch_idx, batch in enumerate(tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")):\n                try:\n                    # Move to device\n                    batch = batch.to(device)\n                    \n                    # Handle NaN values\n                    if torch.isnan(batch).any():\n                        batch = torch.nan_to_num(batch, nan=0.0)\n                    \n                    # Training step\n                    loss = diffusion.train_step(batch, optimizer)\n                    epoch_loss += loss\n                    batch_count += 1\n                    \n                except Exception as e:\n                    print(f\"Error in batch {batch_idx}: {e}\")\n                    continue\n            \n            # Calculate average loss\n            avg_loss = epoch_loss / max(1, batch_count)\n            print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.6f}\")\n            \n            # Save checkpoint\n            if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n                checkpoint_dir = os.path.join(output_dir, \"checkpoints\")\n                os.makedirs(checkpoint_dir, exist_ok=True)\n                \n                checkpoint_path = os.path.join(checkpoint_dir, f\"diffusion_epoch{epoch+1}.pt\")\n                torch.save({\n                    'epoch': epoch,\n                    'model_state_dict': model.state_dict(),\n                    'optimizer_state_dict': optimizer.state_dict(),\n                    'loss': avg_loss,\n                }, checkpoint_path)\n                \n                print(f\"Saved checkpoint to {checkpoint_path}\")\n        \n        # Replace sample method with fixed version\n        diffusion.sample = fixed_sample.__get__(diffusion, type(diffusion))\n        \n        return model, diffusion\n    \n    # Train model\n    model, diffusion = train_diffusion_model_fixed(\n        dataset,\n        epochs=epochs,\n        batch_size=min(4, len(dataset)),\n        device=device\n    )\n    \n    if model is None or diffusion is None:\n        print(\"Training failed!\")\n        return None, None\n    \n    # Step 4: Generate new 3D models\n    print(\"\\n=== Step 4: Generating new 3D models ===\")\n    \n    # Get shape information\n    feature_channels = 32  # Fixed feature channels\n    resolution = target_resolution  # Use the same resolution as training\n    \n    print(f\"Using resolution: {resolution}, feature channels: {feature_channels}\")\n    \n    # Generate samples\n    generate_samples(\n        model,\n        diffusion,\n        output_dir,\n        n_samples=5,\n        resolution=resolution,\n        feature_channels=feature_channels,\n        device=device\n    )\n    \n    print(f\"\\nPipeline complete! Results saved to {output_dir}\")\n    return model, diffusion","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nif __name__ == \"__main__\":\n    # ModelNet40 Dataset\n    modelnet40_path = \"/kaggle/input/modelnet40-princeton-3d-object-dataset/ModelNet40\"\n    \n    model, diffusion = triplane_diffusion_pipeline(\n        modelnet40_path,\n        output_dir=\"fixed_results\",\n        categories=[\"airplane\"],\n        max_models=1,\n        epochs=30\n    )","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}